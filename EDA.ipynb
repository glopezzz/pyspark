{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ebf82719",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import Window\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a3ca2bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.0.0'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder.appName(\"iot\").getOrCreate()\n",
    ")\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "spark.sparkContext.version\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9050f24",
   "metadata": {},
   "source": [
    "Credits to Anton T. Ruberts:\n",
    "https://www.youtube.com/watch?v=2LG2hUQxLmA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378cc966",
   "metadata": {},
   "source": [
    "Dataset from\n",
    "https://www.kaggle.com/datasets/agungpambudi/network-malware-detection-connection-analysis?resource=download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f9f64199",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\I'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\I'\n",
      "C:\\Users\\gabyl\\AppData\\Local\\Temp\\ipykernel_5028\\1134689689.py:1: SyntaxWarning: invalid escape sequence '\\I'\n",
      "  file_path = r\"C:\\Users\\gabyl\\OneDrive\\Desktop\\Proyectos\\pyspark\\data\" + \"\\IoT-Malware.csv\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+---------------+---------+---------------+---------+-----+-------+--------+----------+----------+----------+----------+----------+------------+-------+---------+-------------+---------+-------------+--------------+---------+--------------------+\n",
      "|                 ts|               uid|      id.orig_h|id.orig_p|      id.resp_h|id.resp_p|proto|service|duration|orig_bytes|resp_bytes|conn_state|local_orig|local_resp|missed_bytes|history|orig_pkts|orig_ip_bytes|resp_pkts|resp_ip_bytes|tunnel_parents|    label|      detailed-label|\n",
      "+-------------------+------------------+---------------+---------+---------------+---------+-----+-------+--------+----------+----------+----------+----------+----------+------------+-------+---------+-------------+---------+-------------+--------------+---------+--------------------+\n",
      "|1.525879831015811E9|CUmrqr4svHuSXJy5z7|192.168.100.103|  51524.0| 65.127.233.163|     23.0|  tcp|      -|2.999051|         0|         0|        S0|         -|         -|         0.0|      S|      3.0|        180.0|      0.0|          0.0|             -|Malicious|PartOfAHorizontal...|\n",
      "|1.525879831025055E9|CH98aB3s1kJeq6SFOc|192.168.100.103|  56305.0|  63.150.16.171|     23.0|  tcp|      -|       -|         -|         -|        S0|         -|         -|         0.0|      S|      1.0|         60.0|      0.0|          0.0|             -|Malicious|PartOfAHorizontal...|\n",
      "|1.525879831045045E9| C3GBTkINvXNjVGtN5|192.168.100.103|  41101.0|   111.40.23.49|     23.0|  tcp|      -|       -|         -|         -|        S0|         -|         -|         0.0|      S|      1.0|         60.0|      0.0|          0.0|             -|Malicious|PartOfAHorizontal...|\n",
      "| 1.52587983201624E9| CDe43c1PtgynajGI6|192.168.100.103|  60905.0|131.174.215.147|     23.0|  tcp|      -|2.998796|         0|         0|        S0|         -|         -|         0.0|      S|      3.0|        180.0|      0.0|          0.0|             -|Malicious|PartOfAHorizontal...|\n",
      "|1.525879832024985E9|CJaDcG3MZzvf1YVYI4|192.168.100.103|  44301.0|    91.42.47.63|     23.0|  tcp|      -|       -|         -|         -|        S0|         -|         -|         0.0|      S|      1.0|         60.0|      0.0|          0.0|             -|Malicious|PartOfAHorizontal...|\n",
      "+-------------------+------------------+---------------+---------+---------------+---------+-----+-------+--------+----------+----------+----------+----------+----------+------------+-------+---------+-------------+---------+-------------+--------------+---------+--------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "file_path = r\"C:\\Users\\gabyl\\OneDrive\\Desktop\\Proyectos\\pyspark\\data\" + \"\\IoT-Malware.csv\"\n",
    "df = spark.read.option('delimiter', '|').csv(file_path, inferSchema=True, header=True)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "815a825b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ts: double (nullable = true)\n",
      " |-- uid: string (nullable = true)\n",
      " |-- id.orig_h: string (nullable = true)\n",
      " |-- id.orig_p: double (nullable = true)\n",
      " |-- id.resp_h: string (nullable = true)\n",
      " |-- id.resp_p: double (nullable = true)\n",
      " |-- proto: string (nullable = true)\n",
      " |-- service: string (nullable = true)\n",
      " |-- duration: string (nullable = true)\n",
      " |-- orig_bytes: string (nullable = true)\n",
      " |-- resp_bytes: string (nullable = true)\n",
      " |-- conn_state: string (nullable = true)\n",
      " |-- local_orig: string (nullable = true)\n",
      " |-- local_resp: string (nullable = true)\n",
      " |-- missed_bytes: double (nullable = true)\n",
      " |-- history: string (nullable = true)\n",
      " |-- orig_pkts: double (nullable = true)\n",
      " |-- orig_ip_bytes: double (nullable = true)\n",
      " |-- resp_pkts: double (nullable = true)\n",
      " |-- resp_ip_bytes: double (nullable = true)\n",
      " |-- tunnel_parents: string (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      " |-- detailed-label: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce8b897",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "66c0342d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"dt\", F.from_unixtime(\"ts\")).withColumn(\"dt\", F.to_timestamp(\"dt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e736bb2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|                 dt|\n",
      "+-------------------+\n",
      "|2018-05-09 17:30:31|\n",
      "|2018-05-09 17:30:31|\n",
      "|2018-05-09 17:30:31|\n",
      "+-------------------+\n",
      "only showing top 3 rows\n"
     ]
    }
   ],
   "source": [
    "df.select(\"dt\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec094d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumnsRenamed(\n",
    "    {\n",
    "        \"id.orig_h\": \"source_ip\",\n",
    "        \"id.orig_p\": \"source_port\",\n",
    "        \"id.resp_h\": \"dest_ip\",\n",
    "        \"id.resp_p\": \"dest_port\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "644286f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ts',\n",
       " 'uid',\n",
       " 'source_ip',\n",
       " 'source_port',\n",
       " 'dest_ip',\n",
       " 'dest_port',\n",
       " 'proto',\n",
       " 'service',\n",
       " 'duration',\n",
       " 'orig_bytes',\n",
       " 'resp_bytes',\n",
       " 'conn_state',\n",
       " 'local_orig',\n",
       " 'local_resp',\n",
       " 'missed_bytes',\n",
       " 'history',\n",
       " 'orig_pkts',\n",
       " 'orig_ip_bytes',\n",
       " 'resp_pkts',\n",
       " 'resp_ip_bytes',\n",
       " 'tunnel_parents',\n",
       " 'label',\n",
       " 'detailed-label',\n",
       " 'dt']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f69d505b",
   "metadata": {},
   "source": [
    "### Dataset quality checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c71a2403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+\n",
      "|           min_date|           max_date|\n",
      "+-------------------+-------------------+\n",
      "|2018-05-09 17:30:31|2018-05-14 09:24:43|\n",
      "+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.agg(\n",
    "    F.min(\"dt\").alias(\"min_date\"),\n",
    "    F.max(\"dt\").alias(\"max_date\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a75bd5d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1008748, 24)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df shape:\n",
    "df.count(), len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c87051e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+-------+---------+-----+-------+--------+----------+----------+----------+----------+----------+------------+-------+---------+-------------+---------+-------------+--------------+-----+--------------+\n",
      "|source_ip|source_port|dest_ip|dest_port|proto|service|duration|orig_bytes|resp_bytes|conn_state|local_orig|local_resp|missed_bytes|history|orig_pkts|orig_ip_bytes|resp_pkts|resp_ip_bytes|tunnel_parents|label|detailed-label|\n",
      "+---------+-----------+-------+---------+-----+-------+--------+----------+----------+----------+----------+----------+------------+-------+---------+-------------+---------+-------------+--------------+-----+--------------+\n",
      "|    15004|      28243| 597107|    65426|    3|      5|   16650|       171|       479|        11|         1|         1|           1|    126|       54|         1249|       69|         1141|             1|    2|             3|\n",
      "+---------+-----------+-------+---------+-----+-------+--------+----------+----------+----------+----------+----------+------------+-------+---------+-------------+---------+-------------+--------------+-----+--------------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "to_analyse = [\n",
    "    \"source_ip\",\n",
    "    \"source_port\",\n",
    "    \"dest_ip\",\n",
    "    \"dest_port\",\n",
    "    \"proto\",\n",
    "    \"service\",\n",
    "    \"duration\",\n",
    "    \"orig_bytes\",\n",
    "    \"resp_bytes\",\n",
    "    \"conn_state\",\n",
    "    \"local_orig\",\n",
    "    \"local_resp\",\n",
    "    \"missed_bytes\",\n",
    "    \"history\",\n",
    "    \"orig_pkts\",\n",
    "    \"orig_ip_bytes\",\n",
    "    \"resp_pkts\",\n",
    "    \"resp_ip_bytes\",\n",
    "    \"tunnel_parents\",\n",
    "    \"label\",\n",
    "    \"detailed-label\",\n",
    "]\n",
    "\n",
    "unique_counts = df.agg(\n",
    "        *(F.countDistinct(\n",
    "            F.col(c)\n",
    "            ).alias(c) for c in to_analyse))\n",
    "print(unique_counts.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f7a0a6b5",
   "metadata": {},
   "outputs": [
    {
     "ename": "PySparkAttributeError",
     "evalue": "[ATTRIBUTE_NOT_SUPPORTED] Attribute `asDict` is not supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPySparkAttributeError\u001b[39m                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# unique_counts = unique_counts.first()\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m static_cols = [c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[43munique_counts\u001b[49m\u001b[43m.\u001b[49m\u001b[43masDict\u001b[49m() \u001b[38;5;28;01mif\u001b[39;00m unique_counts[c] == \u001b[32m1\u001b[39m]\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(static_cols)\n\u001b[32m      4\u001b[39m df = df.drop(*static_cols)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pyspark\\sql\\classic\\dataframe.py:971\u001b[39m, in \u001b[36mDataFrame.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m    969\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name: \u001b[38;5;28mstr\u001b[39m) -> Column:\n\u001b[32m    970\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns:\n\u001b[32m--> \u001b[39m\u001b[32m971\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m PySparkAttributeError(\n\u001b[32m    972\u001b[39m             errorClass=\u001b[33m\"\u001b[39m\u001b[33mATTRIBUTE_NOT_SUPPORTED\u001b[39m\u001b[33m\"\u001b[39m, messageParameters={\u001b[33m\"\u001b[39m\u001b[33mattr_name\u001b[39m\u001b[33m\"\u001b[39m: name}\n\u001b[32m    973\u001b[39m         )\n\u001b[32m    974\u001b[39m     jc = \u001b[38;5;28mself\u001b[39m._jdf.apply(name)\n\u001b[32m    975\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m Column(jc)\n",
      "\u001b[31mPySparkAttributeError\u001b[39m: [ATTRIBUTE_NOT_SUPPORTED] Attribute `asDict` is not supported."
     ]
    }
   ],
   "source": [
    "# unique_counts = unique_counts.first()\n",
    "static_cols = [c for c in unique_counts.asDict() if unique_counts[c] == 1]\n",
    "print(static_cols)\n",
    "df = df.drop(*static_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef9afd0",
   "metadata": {},
   "source": [
    "### Count Nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bfaec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---------+-----------+-------+---------+-----+-------+--------+----------+----------+----------+-------+---------+-------------+---------+-------------+-----+--------------+---+\n",
      "| ts|uid|source_ip|source_port|dest_ip|dest_port|proto|service|duration|orig_bytes|resp_bytes|conn_state|history|orig_pkts|orig_ip_bytes|resp_pkts|resp_ip_bytes|label|detailed-label| dt|\n",
      "+---+---+---------+-----------+-------+---------+-----+-------+--------+----------+----------+----------+-------+---------+-------------+---------+-------------+-----+--------------+---+\n",
      "|  0|  0|        0|          0|      0|        0|    0|1005507|  796300|    796300|    796300|         0|  17421|        0|            0|        0|            0|    0|        469275|  0|\n",
      "+---+---+---------+-----------+-------+---------+-----+-------+--------+----------+----------+----------+-------+---------+-------------+---------+-------------+-----+--------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.replace(\"-\", None)\n",
    "non_static_cols = [c for c in df.columns if c not in static_cols]\n",
    "df.select(\n",
    "    [F.count(F.when(F.col(c).isNull(), c)).alias(c) for c in non_static_cols]\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e57fd9",
   "metadata": {},
   "source": [
    "### Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66027bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumns(\n",
    "    {\n",
    "        \"day\": F.date_trunc(\"day\", \"dt\"),\n",
    "        \"hour\": F.date_trunc(\"hour\", \"dt\"),\n",
    "        \"minute\": F.date_trunc(\"minute\", \"dt\"),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3610a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "hour",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "label",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "counts",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "ea4a121a-75f7-4bd1-b38f-9ca05ef38468",
       "rows": [
        [
         "0",
         "2018-05-09 17:00:00",
         "Benign",
         "2197"
        ],
        [
         "1",
         "2018-05-09 17:00:00",
         "Malicious",
         "2623"
        ],
        [
         "2",
         "2018-05-09 18:00:00",
         "Malicious",
         "5420"
        ],
        [
         "3",
         "2018-05-09 18:00:00",
         "Benign",
         "4346"
        ],
        [
         "4",
         "2018-05-09 19:00:00",
         "Malicious",
         "5402"
        ],
        [
         "5",
         "2018-05-09 19:00:00",
         "Benign",
         "4329"
        ],
        [
         "6",
         "2018-05-09 20:00:00",
         "Malicious",
         "5557"
        ],
        [
         "7",
         "2018-05-09 20:00:00",
         "Benign",
         "4326"
        ],
        [
         "8",
         "2018-05-09 21:00:00",
         "Benign",
         "4324"
        ],
        [
         "9",
         "2018-05-09 21:00:00",
         "Malicious",
         "5431"
        ],
        [
         "10",
         "2018-05-09 22:00:00",
         "Malicious",
         "5507"
        ],
        [
         "11",
         "2018-05-09 22:00:00",
         "Benign",
         "4298"
        ],
        [
         "12",
         "2018-05-09 23:00:00",
         "Benign",
         "4310"
        ],
        [
         "13",
         "2018-05-09 23:00:00",
         "Malicious",
         "5125"
        ],
        [
         "14",
         "2018-05-10 00:00:00",
         "Malicious",
         "5130"
        ],
        [
         "15",
         "2018-05-10 00:00:00",
         "Benign",
         "4408"
        ],
        [
         "16",
         "2018-05-10 01:00:00",
         "Malicious",
         "5094"
        ],
        [
         "17",
         "2018-05-10 01:00:00",
         "Benign",
         "4296"
        ],
        [
         "18",
         "2018-05-10 02:00:00",
         "Benign",
         "4329"
        ],
        [
         "19",
         "2018-05-10 02:00:00",
         "Malicious",
         "5238"
        ],
        [
         "20",
         "2018-05-10 03:00:00",
         "Malicious",
         "5053"
        ],
        [
         "21",
         "2018-05-10 03:00:00",
         "Benign",
         "4322"
        ],
        [
         "22",
         "2018-05-10 04:00:00",
         "Benign",
         "4291"
        ],
        [
         "23",
         "2018-05-10 04:00:00",
         "Malicious",
         "5212"
        ],
        [
         "24",
         "2018-05-10 05:00:00",
         "Malicious",
         "5153"
        ],
        [
         "25",
         "2018-05-10 05:00:00",
         "Benign",
         "4416"
        ],
        [
         "26",
         "2018-05-10 06:00:00",
         "Malicious",
         "5041"
        ],
        [
         "27",
         "2018-05-10 06:00:00",
         "Benign",
         "4299"
        ],
        [
         "28",
         "2018-05-10 07:00:00",
         "Malicious",
         "4924"
        ],
        [
         "29",
         "2018-05-10 07:00:00",
         "Benign",
         "4357"
        ],
        [
         "30",
         "2018-05-10 08:00:00",
         "Benign",
         "4365"
        ],
        [
         "31",
         "2018-05-10 08:00:00",
         "Malicious",
         "5076"
        ],
        [
         "32",
         "2018-05-10 09:00:00",
         "Benign",
         "4356"
        ],
        [
         "33",
         "2018-05-10 09:00:00",
         "Malicious",
         "5191"
        ],
        [
         "34",
         "2018-05-10 10:00:00",
         "Benign",
         "4412"
        ],
        [
         "35",
         "2018-05-10 10:00:00",
         "Malicious",
         "5112"
        ],
        [
         "36",
         "2018-05-10 11:00:00",
         "Malicious",
         "5101"
        ],
        [
         "37",
         "2018-05-10 11:00:00",
         "Benign",
         "4349"
        ],
        [
         "38",
         "2018-05-10 12:00:00",
         "Malicious",
         "5128"
        ],
        [
         "39",
         "2018-05-10 12:00:00",
         "Benign",
         "4330"
        ],
        [
         "40",
         "2018-05-10 13:00:00",
         "Benign",
         "4384"
        ],
        [
         "41",
         "2018-05-10 13:00:00",
         "Malicious",
         "5021"
        ],
        [
         "42",
         "2018-05-10 14:00:00",
         "Malicious",
         "5042"
        ],
        [
         "43",
         "2018-05-10 14:00:00",
         "Benign",
         "4359"
        ],
        [
         "44",
         "2018-05-10 15:00:00",
         "Benign",
         "4266"
        ],
        [
         "45",
         "2018-05-10 15:00:00",
         "Malicious",
         "5225"
        ],
        [
         "46",
         "2018-05-10 16:00:00",
         "Benign",
         "4322"
        ],
        [
         "47",
         "2018-05-10 16:00:00",
         "Malicious",
         "5212"
        ],
        [
         "48",
         "2018-05-10 17:00:00",
         "Benign",
         "4430"
        ],
        [
         "49",
         "2018-05-10 17:00:00",
         "Malicious",
         "5184"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 226
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hour</th>\n",
       "      <th>label</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-05-09 17:00:00</td>\n",
       "      <td>Benign</td>\n",
       "      <td>2197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-05-09 17:00:00</td>\n",
       "      <td>Malicious</td>\n",
       "      <td>2623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018-05-09 18:00:00</td>\n",
       "      <td>Malicious</td>\n",
       "      <td>5420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2018-05-09 18:00:00</td>\n",
       "      <td>Benign</td>\n",
       "      <td>4346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2018-05-09 19:00:00</td>\n",
       "      <td>Malicious</td>\n",
       "      <td>5402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>2018-05-14 07:00:00</td>\n",
       "      <td>Malicious</td>\n",
       "      <td>4399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>2018-05-14 08:00:00</td>\n",
       "      <td>Benign</td>\n",
       "      <td>4036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>2018-05-14 08:00:00</td>\n",
       "      <td>Malicious</td>\n",
       "      <td>4325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>2018-05-14 09:00:00</td>\n",
       "      <td>Benign</td>\n",
       "      <td>1608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>2018-05-14 09:00:00</td>\n",
       "      <td>Malicious</td>\n",
       "      <td>1823</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>226 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   hour      label  counts\n",
       "0   2018-05-09 17:00:00     Benign    2197\n",
       "1   2018-05-09 17:00:00  Malicious    2623\n",
       "2   2018-05-09 18:00:00  Malicious    5420\n",
       "3   2018-05-09 18:00:00     Benign    4346\n",
       "4   2018-05-09 19:00:00  Malicious    5402\n",
       "..                  ...        ...     ...\n",
       "221 2018-05-14 07:00:00  Malicious    4399\n",
       "222 2018-05-14 08:00:00     Benign    4036\n",
       "223 2018-05-14 08:00:00  Malicious    4325\n",
       "224 2018-05-14 09:00:00     Benign    1608\n",
       "225 2018-05-14 09:00:00  Malicious    1823\n",
       "\n",
       "[226 rows x 3 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupBy([\"hour\", \"label\"]).agg(F.count(\"uid\").alias('counts')).orderBy(\"hour\").toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48bf4ae",
   "metadata": {},
   "source": [
    "### Univariate Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3a0eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-------+\n",
      "|proto| count|percent|\n",
      "+-----+------+-------+\n",
      "| icmp| 17421| 0.0173|\n",
      "|  udp|408193| 0.4047|\n",
      "|  tcp|583134| 0.5781|\n",
      "+-----+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inter_df = df.groupBy(\"proto\").count().alias(\"count\").orderBy(\"count\", ascending=True)\n",
    "inter_df = inter_df.withColumn(\n",
    "    \"percent\", F.round(F.col(\"count\")/F.sum(F.col(\"count\")).over(Window.partitionBy()), 4)\n",
    "    )\n",
    "inter_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e31da929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-------+\n",
      "|proto| count|percent|\n",
      "+-----+------+-------+\n",
      "|  tcp|583134| 0.5781|\n",
      "|  udp|408193| 0.4047|\n",
      "| icmp| 17421| 0.0173|\n",
      "+-----+------+-------+\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Mime type rendering requires nbformat>=4.2.0 but it is not installed",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m categorical_cols = [\u001b[33m\"\u001b[39m\u001b[33mproto\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mservice\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mconn_state\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mhistory\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m categorical_cols:\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     \u001b[43mcounts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mcounts\u001b[39m\u001b[34m(df, var)\u001b[39m\n\u001b[32m      6\u001b[39m var_counts.show()\n\u001b[32m      7\u001b[39m fig = px.bar(var_counts.toPandas(), x=var, y=\u001b[33m\"\u001b[39m\u001b[33mcount\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[43mfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python312\\Lib\\site-packages\\plotly\\basedatatypes.py:3420\u001b[39m, in \u001b[36mBaseFigure.show\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   3387\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3388\u001b[39m \u001b[33;03mShow a figure using either the default renderer(s) or the renderer(s)\u001b[39;00m\n\u001b[32m   3389\u001b[39m \u001b[33;03mspecified by the renderer argument\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   3416\u001b[39m \u001b[33;03mNone\u001b[39;00m\n\u001b[32m   3417\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3418\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplotly\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpio\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3420\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python312\\Lib\\site-packages\\plotly\\io\\_renderers.py:415\u001b[39m, in \u001b[36mshow\u001b[39m\u001b[34m(fig, renderer, validate, **kwargs)\u001b[39m\n\u001b[32m    410\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    411\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mMime type rendering requires ipython but it is not installed\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    412\u001b[39m     )\n\u001b[32m    414\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m nbformat \u001b[38;5;129;01mor\u001b[39;00m Version(nbformat.__version__) < Version(\u001b[33m\"\u001b[39m\u001b[33m4.2.0\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m415\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    416\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mMime type rendering requires nbformat>=4.2.0 but it is not installed\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    417\u001b[39m     )\n\u001b[32m    419\u001b[39m display_jupyter_version_warnings()\n\u001b[32m    421\u001b[39m ipython_display.display(bundle, raw=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mValueError\u001b[39m: Mime type rendering requires nbformat>=4.2.0 but it is not installed"
     ]
    }
   ],
   "source": [
    "def counts(df, var):\n",
    "    var_counts = df.groupBy(var).count().orderBy(\"count\", ascending=False)\n",
    "    var_counts = var_counts.withColumn(\n",
    "        \"percent\", F.round(F.col(\"count\")/F.sum(F.col(\"count\")).over(Window.partitionBy()), 4)\n",
    "    )\n",
    "    var_counts.show()\n",
    "    fig = px.bar(var_counts.toPandas(), x=var, y=\"count\")\n",
    "    fig.show()\n",
    "\n",
    "categorical_cols = [\"proto\", \"service\", \"conn_state\", \"history\", \"label\"]\n",
    "for col in categorical_cols:\n",
    "    counts(df,col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b456fd54",
   "metadata": {},
   "source": [
    "## Prepare for Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e930299",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e14449f8",
   "metadata": {},
   "source": [
    "## Full Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606d8a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------------+---------------+-----------+---------------+---------+-----+-------+---------+----------+----------+----------+-------+---------+-------------+---------+-------------+---------+--------------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n",
      "|                 ts|               uid|      source_ip|source_port|        dest_ip|dest_port|proto|service| duration|orig_bytes|resp_bytes|conn_state|history|orig_pkts|orig_ip_bytes|resp_pkts|resp_ip_bytes|    label|      detailed-label|                 dt|                day|               hour|             minute|             second|\n",
      "+-------------------+------------------+---------------+-----------+---------------+---------+-----+-------+---------+----------+----------+----------+-------+---------+-------------+---------+-------------+---------+--------------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n",
      "|1.525879831015811E9|CUmrqr4svHuSXJy5z7|192.168.100.103|    51524.0| 65.127.233.163|     23.0|  tcp|missing| 2.999051|       0.0|       0.0|        S0|      S|      3.0|        180.0|      0.0|          0.0|Malicious|PartOfAHorizontal...|2018-05-09 17:30:31|2018-05-09 00:00:00|2018-05-09 17:00:00|2018-05-09 17:30:00|2018-05-09 17:30:31|\n",
      "|1.525879831025055E9|CH98aB3s1kJeq6SFOc|192.168.100.103|    56305.0|  63.150.16.171|     23.0|  tcp|missing|-999999.0| -999999.0| -999999.0|        S0|      S|      1.0|         60.0|      0.0|          0.0|Malicious|PartOfAHorizontal...|2018-05-09 17:30:31|2018-05-09 00:00:00|2018-05-09 17:00:00|2018-05-09 17:30:00|2018-05-09 17:30:31|\n",
      "|1.525879831045045E9| C3GBTkINvXNjVGtN5|192.168.100.103|    41101.0|   111.40.23.49|     23.0|  tcp|missing|-999999.0| -999999.0| -999999.0|        S0|      S|      1.0|         60.0|      0.0|          0.0|Malicious|PartOfAHorizontal...|2018-05-09 17:30:31|2018-05-09 00:00:00|2018-05-09 17:00:00|2018-05-09 17:30:00|2018-05-09 17:30:31|\n",
      "| 1.52587983201624E9| CDe43c1PtgynajGI6|192.168.100.103|    60905.0|131.174.215.147|     23.0|  tcp|missing| 2.998796|       0.0|       0.0|        S0|      S|      3.0|        180.0|      0.0|          0.0|Malicious|PartOfAHorizontal...|2018-05-09 17:30:32|2018-05-09 00:00:00|2018-05-09 17:00:00|2018-05-09 17:30:00|2018-05-09 17:30:32|\n",
      "|1.525879832024985E9|CJaDcG3MZzvf1YVYI4|192.168.100.103|    44301.0|    91.42.47.63|     23.0|  tcp|missing|-999999.0| -999999.0| -999999.0|        S0|      S|      1.0|         60.0|      0.0|          0.0|Malicious|PartOfAHorizontal...|2018-05-09 17:30:32|2018-05-09 00:00:00|2018-05-09 17:00:00|2018-05-09 17:30:00|2018-05-09 17:30:32|\n",
      "|1.525879832044975E9|CMBrup3BLXivSp4Avc|192.168.100.103|    50244.0|120.210.108.200|     23.0|  tcp|missing|-999999.0| -999999.0| -999999.0|        S0|      S|      1.0|         60.0|      0.0|          0.0|Malicious|PartOfAHorizontal...|2018-05-09 17:30:32|2018-05-09 00:00:00|2018-05-09 17:00:00|2018-05-09 17:30:00|2018-05-09 17:30:32|\n",
      "|1.525879833016171E9|CfHl9r3XMYtDQRrHnh|192.168.100.103|    34243.0|   147.7.65.203|  49560.0|  tcp|missing| 2.998804|       0.0|       0.0|        S0|      S|      3.0|        180.0|      0.0|          0.0|   Benign|                NULL|2018-05-09 17:30:33|2018-05-09 00:00:00|2018-05-09 17:00:00|2018-05-09 17:30:00|2018-05-09 17:30:33|\n",
      "|1.525879833044906E9|C7USrA15nFVkniMqC5|192.168.100.103|    34840.0|   145.164.35.6|  21288.0|  tcp|missing|-999999.0| -999999.0| -999999.0|        S0|      S|      1.0|         60.0|      0.0|          0.0|   Benign|                NULL|2018-05-09 17:30:33|2018-05-09 00:00:00|2018-05-09 17:00:00|2018-05-09 17:30:00|2018-05-09 17:30:33|\n",
      "|1.525879834024847E9| CDtZ4so7bHKpIeCmi|192.168.100.103|    58525.0| 177.75.151.125|     23.0|  tcp|missing|-999999.0| -999999.0| -999999.0|        S0|      S|      1.0|         60.0|      0.0|          0.0|Malicious|PartOfAHorizontal...|2018-05-09 17:30:34|2018-05-09 00:00:00|2018-05-09 17:00:00|2018-05-09 17:30:00|2018-05-09 17:30:34|\n",
      "|1.525879834045086E9| Cj9lm7amKnWiNDh9c|192.168.100.103|    43849.0|  94.145.70.119|   8080.0|  tcp|missing|-999999.0| -999999.0| -999999.0|        S0|      S|      1.0|         60.0|      0.0|          0.0|Malicious|PartOfAHorizontal...|2018-05-09 17:30:34|2018-05-09 00:00:00|2018-05-09 17:00:00|2018-05-09 17:30:00|2018-05-09 17:30:34|\n",
      "|1.525879836044966E9|CgeVK21nm6PyVgJ4ah|192.168.100.103|    40973.0| 62.239.193.205|   8080.0|  tcp|missing|-999999.0| -999999.0| -999999.0|        S0|      S|      1.0|         60.0|      0.0|          0.0|Malicious|PartOfAHorizontal...|2018-05-09 17:30:36|2018-05-09 00:00:00|2018-05-09 17:00:00|2018-05-09 17:30:00|2018-05-09 17:30:36|\n",
      "|1.525879837005652E9|CFXxTW19F3sluNEkJk|192.168.100.103|    52259.0|193.189.106.178|     23.0|  tcp|missing|   2.9993|       0.0|       0.0|        S0|      S|      3.0|        180.0|      0.0|          0.0|Malicious|PartOfAHorizontal...|2018-05-09 17:30:37|2018-05-09 00:00:00|2018-05-09 17:00:00|2018-05-09 17:30:00|2018-05-09 17:30:37|\n",
      "|1.525879838006081E9|CRRl9U39PzvL3OiS61|192.168.100.103|    42207.0|  10.169.219.98|   8080.0|  tcp|missing| 2.993548|       0.0|       0.0|        S0|      S|      3.0|        180.0|      0.0|          0.0|Malicious|PartOfAHorizontal...|2018-05-09 17:30:38|2018-05-09 00:00:00|2018-05-09 17:00:00|2018-05-09 17:30:00|2018-05-09 17:30:38|\n",
      "|1.525879838024838E9|C9hk4p2z9Y6AvlzLff|192.168.100.103|    51524.0| 65.127.233.163|     23.0|  tcp|missing|-999999.0| -999999.0| -999999.0|        S0|      S|      1.0|         60.0|      0.0|          0.0|Malicious|PartOfAHorizontal...|2018-05-09 17:30:38|2018-05-09 00:00:00|2018-05-09 17:00:00|2018-05-09 17:30:00|2018-05-09 17:30:38|\n",
      "|1.525879839006262E9|Cx7t1f1wqRES8kiTq6|192.168.100.103|    40459.0|212.110.180.188|     23.0|  tcp|missing| 2.998807|       0.0|       0.0|        S0|      S|      3.0|        180.0|      0.0|          0.0|Malicious|PartOfAHorizontal...|2018-05-09 17:30:39|2018-05-09 00:00:00|2018-05-09 17:00:00|2018-05-09 17:30:00|2018-05-09 17:30:39|\n",
      "|1.525879839025003E9| C3AFse2JfKAgalsik|192.168.100.103|    60905.0|131.174.215.147|     23.0|  tcp|missing|-999999.0| -999999.0| -999999.0|        S0|      S|      1.0|         60.0|      0.0|          0.0|Malicious|PartOfAHorizontal...|2018-05-09 17:30:39|2018-05-09 00:00:00|2018-05-09 17:00:00|2018-05-09 17:30:00|2018-05-09 17:30:39|\n",
      "|1.525879839044992E9| CdQJe43wv9ipLJklh|192.168.100.103|    56305.0|  63.150.16.171|     23.0|  tcp|missing|-999999.0| -999999.0| -999999.0|        S0|      S|      1.0|         60.0|      0.0|          0.0|Malicious|PartOfAHorizontal...|2018-05-09 17:30:39|2018-05-09 00:00:00|2018-05-09 17:00:00|2018-05-09 17:30:00|2018-05-09 17:30:39|\n",
      "|1.525879840024934E9|C3ajsm4byxMkPys9Wa|192.168.100.103|    34243.0|   147.7.65.203|  49560.0|  tcp|missing|-999999.0| -999999.0| -999999.0|        S0|      S|      1.0|         60.0|      0.0|          0.0|   Benign|                NULL|2018-05-09 17:30:40|2018-05-09 00:00:00|2018-05-09 17:00:00|2018-05-09 17:30:00|2018-05-09 17:30:40|\n",
      "|1.525879840044922E9|Cta8kR3Zfh09uPo77k|192.168.100.103|    44301.0|    91.42.47.63|     23.0|  tcp|missing|-999999.0| -999999.0| -999999.0|        S0|      S|      1.0|         60.0|      0.0|          0.0|Malicious|PartOfAHorizontal...|2018-05-09 17:30:40|2018-05-09 00:00:00|2018-05-09 17:00:00|2018-05-09 17:30:00|2018-05-09 17:30:40|\n",
      "|1.525879841005875E9|CRwLRg1TuvUDjEHuI5|192.168.100.103|    39813.0|  167.185.30.24|     23.0|  tcp|missing|  2.99905|       0.0|       0.0|        S0|      S|      3.0|        180.0|      0.0|          0.0|Malicious|PartOfAHorizontal...|2018-05-09 17:30:41|2018-05-09 00:00:00|2018-05-09 17:00:00|2018-05-09 17:30:00|2018-05-09 17:30:41|\n",
      "+-------------------+------------------+---------------+-----------+---------------+---------+-----+-------+---------+----------+----------+----------+-------+---------+-------------+---------+-------------+---------+--------------------+-------------------+-------------------+-------------------+-------------------+-------------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "static_cols = [\"local_orig\", \"local_resp\", \"missed_bytes\", \"tunnel_parents\"]\n",
    "recast_cols = {\n",
    "    \"duration\": F.col(\"duration\").cast(\"double\"),\n",
    "    \"orig_bytes\": F.col(\"orig_bytes\").cast(\"double\"),\n",
    "    \"resp_bytes\": F.col(\"resp_bytes\").cast(\"double\"),\n",
    "    \"orig_ip_bytes\": F.col(\"orig_ip_bytes\").cast(\"double\"),\n",
    "    \"orig_pkts\": F.col(\"orig_pkts\").cast(\"double\"),\n",
    "    \"resp_pkts\": F.col(\"resp_pkts\").cast(\"double\"),\n",
    "    \"resp_ip_bytes\": F.col(\"resp_ip_bytes\").cast(\"double\"),\n",
    "}\n",
    "\n",
    "fill_vals = {\n",
    "    \"duration\": -999999,\n",
    "    \"orig_bytes\": -999999,\n",
    "    \"resp_bytes\": -999999,\n",
    "    \"orig_pkts\": -999999,\n",
    "    \"orig_ip_bytes\": -999999,\n",
    "    \"resp_pkts\": -999999,\n",
    "    \"resp_ip_bytes\": -999999,\n",
    "    \"history\": \"missing\",\n",
    "    \"proto\": \"missing\",\n",
    "    \"service\": \"missing\",\n",
    "    \"conn_state\": \"missing\",\n",
    "}\n",
    "\n",
    "preprocessed_data = (\n",
    "    spark.read.option(\"delimiter\", '|')\n",
    "    .csv(file_path, inferSchema=True, header=True)\n",
    "    .withColumn(\"dt\", F.to_timestamp(F.from_unixtime(\"ts\")))\n",
    "    .withColumns(\n",
    "        {\n",
    "            \"day\": F.date_trunc(\"day\",\"dt\"),\n",
    "            \"hour\": F.date_trunc(\"hour\",\"dt\"),\n",
    "            \"minute\": F.date_trunc(\"minute\",\"dt\"),\n",
    "            \"second\": F.date_trunc(\"second\",\"dt\"),\n",
    "        }\n",
    "    )\n",
    "    .withColumnsRenamed(\n",
    "        {\n",
    "            \"id.orig_h\": \"source_ip\",\n",
    "            \"id.orig_p\": \"source_port\",\n",
    "            \"id.resp_h\": \"dest_ip\",\n",
    "            \"id.resp_p\": \"dest_port\"\n",
    "        }\n",
    "    )\n",
    "    .drop(*static_cols)\n",
    "    .replace(\"-\", None)\n",
    "    .withColumns(recast_cols)\n",
    "    .fillna(fill_vals)\n",
    ")\n",
    "\n",
    "preprocessed_data.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7282ab",
   "metadata": {},
   "source": [
    "## Write Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da7a442",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o2193.parquet.\n: java.lang.UnsatisfiedLinkError: 'boolean org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(java.lang.String, int)'\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:817)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1415)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1620)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:739)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2078)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2122)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:961)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2078)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2122)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n\tat org.apache.parquet.hadoop.ParquetOutputCommitter.commitJob(ParquetOutputCommitter.java:46)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:194)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\r\n\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:481)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:306)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:189)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:195)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:117)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:115)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:129)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)\r\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\r\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)\r\n\tat scala.util.Try$.apply(Try.scala:217)\r\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\r\n\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\r\n\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\r\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:131)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:192)\r\n\tat org.apache.spark.sql.classic.DataFrameWriter.runCommand(DataFrameWriter.scala:622)\r\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)\r\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:241)\r\n\tat org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:118)\r\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:369)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[81]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m output_dir = \u001b[33mr\u001b[39m\u001b[33m'\u001b[39m\u001b[33mfile:///C:/Users/gabyl/OneDrive/Desktop/Proyectos/pyspark/output/preprocessing\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Your write command remains the same\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/df.parquet\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moverwrite\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pyspark\\sql\\readwriter.py:2003\u001b[39m, in \u001b[36mDataFrameWriter.parquet\u001b[39m\u001b[34m(self, path, mode, partitionBy, compression)\u001b[39m\n\u001b[32m   2001\u001b[39m     \u001b[38;5;28mself\u001b[39m.partitionBy(partitionBy)\n\u001b[32m   2002\u001b[39m \u001b[38;5;28mself\u001b[39m._set_opts(compression=compression)\n\u001b[32m-> \u001b[39m\u001b[32m2003\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jwrite\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python312\\Lib\\site-packages\\py4j\\java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\pyspark\\errors\\exceptions\\captured.py:282\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    279\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpy4j\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprotocol\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Py4JJavaError\n\u001b[32m    281\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    284\u001b[39m     converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python312\\Lib\\site-packages\\py4j\\protocol.py:327\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    325\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    326\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    328\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    329\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    331\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    332\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    333\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling o2193.parquet.\n: java.lang.UnsatisfiedLinkError: 'boolean org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(java.lang.String, int)'\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:817)\r\n\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1415)\r\n\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1620)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:739)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2078)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2122)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:961)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2078)\r\n\tat org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2122)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.getAllCommittedTaskPaths(FileOutputCommitter.java:334)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJobInternal(FileOutputCommitter.java:404)\r\n\tat org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter.commitJob(FileOutputCommitter.java:377)\r\n\tat org.apache.parquet.hadoop.ParquetOutputCommitter.commitJob(ParquetOutputCommitter.java:46)\r\n\tat org.apache.spark.internal.io.HadoopMapReduceCommitProtocol.commitJob(HadoopMapReduceCommitProtocol.scala:194)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$writeAndCommit$3(FileFormatWriter.scala:275)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\r\n\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:481)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:275)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:306)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:189)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:195)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:117)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:115)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:129)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$2(QueryExecution.scala:155)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:162)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:268)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:124)\r\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\r\n\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:124)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:291)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:123)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:77)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:233)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$eagerlyExecuteCommands$1(QueryExecution.scala:155)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\r\n\tat org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$eagerlyExecute$1(QueryExecution.scala:154)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:169)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$3.applyOrElse(QueryExecution.scala:164)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:470)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:470)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:37)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:360)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:356)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:37)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:446)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:164)\r\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyCommandExecuted$1(QueryExecution.scala:126)\r\n\tat scala.util.Try$.apply(Try.scala:217)\r\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\r\n\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\r\n\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\r\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:131)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:192)\r\n\tat org.apache.spark.sql.classic.DataFrameWriter.runCommand(DataFrameWriter.scala:622)\r\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)\r\n\tat org.apache.spark.sql.classic.DataFrameWriter.saveInternal(DataFrameWriter.scala:241)\r\n\tat org.apache.spark.sql.classic.DataFrameWriter.save(DataFrameWriter.scala:118)\r\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:369)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\n"
     ]
    }
   ],
   "source": [
    "# Note the forward slashes and the file:/// prefix\n",
    "output_dir = r'file:///C:/Users/gabyl/OneDrive/Desktop/Proyectos/pyspark/output/preprocessing'\n",
    "\n",
    "# Your write command remains the same\n",
    "df.write.parquet(output_dir + \"/df.parquet\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a386e5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_in = spark.read.parquet(\"processed.pq\")\n",
    "read_in.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
