{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "405eaa17",
   "metadata": {},
   "source": [
    "## California Housing Prices\n",
    "Median house prices for California districts derived from the 1990 census.\n",
    "\n",
    "### About Dataset\n",
    "#### Context\n",
    "This is the dataset used in the second chapter of Aurélien Géron's recent book 'Hands-On Machine learning with Scikit-Learn and TensorFlow'. It serves as an excellent introduction to implementing machine learning algorithms because it requires rudimentary data cleaning, has an easily understandable list of variables and sits at an optimal size between being to toyish and too cumbersome.\n",
    "\n",
    "The data contains information from the 1990 California census. So although it may not help you with predicting current housing prices like the Zillow Zestimate dataset, it does provide an accessible introductory dataset for teaching people about the basics of machine learning.\n",
    "\n",
    "### Content\n",
    "The data pertains to the houses found in a given California district and some summary stats about them based on the 1990 census data. Be warned the data aren't cleaned so there are some preprocessing steps required! The columns are as follows, their names are pretty self explanitory:\n",
    "\n",
    "- longitude\n",
    "- latitude\n",
    "- housing_median_age\n",
    "- total_rooms\n",
    "- total_bedrooms\n",
    "- population\n",
    "- households\n",
    "- median_income\n",
    "- median_house_value\n",
    "- ocean_proximity\n",
    "\n",
    "### Acknowledgements\n",
    "This data was initially featured in the following paper:\n",
    "Pace, R. Kelley, and Ronald Barry. \"Sparse spatial autoregressions.\" Statistics & Probability Letters 33.3 (1997): 291-297.\n",
    "\n",
    "and I encountered it in 'Hands-On Machine learning with Scikit-Learn and TensorFlow' by Aurélien Géron.\n",
    "Aurélien Géron wrote:\n",
    "This dataset is a modified version of the California Housing dataset available from:\n",
    "Luís Torgo's page (University of Porto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7cd6f164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\Gabriel\\.cache\\kagglehub\\datasets\\camnugent\\california-housing-prices\\versions\\1\n",
      "Successfully copied file\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"camnugent/california-housing-prices\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n",
    "fullpath = os.path.join(path, os.listdir(path)[0])\n",
    "\n",
    "# Now lets copy it to the project's folder\n",
    "destination_folder = Path('data')\n",
    "\n",
    "try:\n",
    "    destination_folder.mkdir(parents=True, exist_ok=True)\n",
    "    shutil.copy(fullpath, destination_folder)\n",
    "    print(f\"Successfully copied file\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3ce91928",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName('California_Housing_Prediction')\n",
    "    .master('local[*]')\n",
    "    .config(\"spark.driver.memory\", '4g')\n",
    "    .getOrCreate()\n",
    ")\n",
    "spark.sparkContext.setLogLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b49fef51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n",
      "|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|ocean_proximity|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n",
      "|  -122.23|   37.88|              41.0|      880.0|         129.0|     322.0|     126.0|       8.3252|          452600.0|       NEAR BAY|\n",
      "|  -122.22|   37.86|              21.0|     7099.0|        1106.0|    2401.0|    1138.0|       8.3014|          358500.0|       NEAR BAY|\n",
      "|  -122.24|   37.85|              52.0|     1467.0|         190.0|     496.0|     177.0|       7.2574|          352100.0|       NEAR BAY|\n",
      "|  -122.25|   37.85|              52.0|     1274.0|         235.0|     558.0|     219.0|       5.6431|          341300.0|       NEAR BAY|\n",
      "|  -122.25|   37.85|              52.0|     1627.0|         280.0|     565.0|     259.0|       3.8462|          342200.0|       NEAR BAY|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('./data/housing.csv', header=True, inferSchema=True)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7b5d225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- housing_median_age: double (nullable = true)\n",
      " |-- total_rooms: double (nullable = true)\n",
      " |-- total_bedrooms: double (nullable = true)\n",
      " |-- population: double (nullable = true)\n",
      " |-- households: double (nullable = true)\n",
      " |-- median_income: double (nullable = true)\n",
      " |-- median_house_value: double (nullable = true)\n",
      " |-- ocean_proximity: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b74eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 20640\n"
     ]
    }
   ],
   "source": [
    "print('Total rows:',df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921015d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets crate an unique id column\n",
    "df = df.withColumn('id', F.monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24594e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n",
      "|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|ocean_proximity|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n",
      "|        0|       0|                 0|          0|           207|         0|         0|            0|                 0|              0|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check Nulls in the dataframe\n",
    "df.select(\n",
    "    [F.count(F.when(F.col(col).isNull(), 1)).alias(col) for col in df.columns]\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e464e2d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+\n",
      "|count(DISTINCT ocean_proximity)|\n",
      "+-------------------------------+\n",
      "|                              5|\n",
      "+-------------------------------+\n",
      "\n",
      "+---------------+-----+\n",
      "|ocean_proximity|count|\n",
      "+---------------+-----+\n",
      "|         ISLAND|    5|\n",
      "|     NEAR OCEAN| 2658|\n",
      "|       NEAR BAY| 2290|\n",
      "|      <1H OCEAN| 9136|\n",
      "|         INLAND| 6551|\n",
      "+---------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check categorical column uniques\n",
    "df.select(F.count_distinct(F.col('ocean_proximity'))).show()\n",
    "df.groupBy(\"ocean_proximity\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4e94c8",
   "metadata": {},
   "source": [
    "#### To handle the missing values for the 'total_bedrooms' column, I'm going to train a Linear Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854a7ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the feature types\n",
    "numerical_features = []\n",
    "categorical_features = []\n",
    "for var, type in df.dtypes:\n",
    "    if type in ['int', 'float', 'double']:\n",
    "        numerical_features.append(var)\n",
    "    elif type in ['string']:\n",
    "        categorical_features.append(var)\n",
    "\n",
    "categorical_features_indexed = [col + '_ind' for col in categorical_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f5613c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|df_train Nulls|\n",
      "+--------------+\n",
      "|             0|\n",
      "+--------------+\n",
      "\n",
      "+-------------+\n",
      "|df_test Nulls|\n",
      "+-------------+\n",
      "|          207|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We could actually fill the Null values with a linear regression model\n",
    "df_train = df.where(F.col('total_bedrooms').isNotNull())\n",
    "df_test = df.where(F.col('total_bedrooms').isNull())\n",
    "\n",
    "# Ensure there arent any nulls\n",
    "df_train.select(\n",
    "    F.count(F.when(F.col('total_bedrooms').isNull(), 1)).alias('df_train Nulls')\n",
    ").show()\n",
    "df_test.select(\n",
    "    F.count(F.when(F.col('total_bedrooms').isNull(), 1)).alias('df_test Nulls')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a0d483a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, MinMaxScaler\n",
    "from pyspark.ml.pipeline import Pipeline\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# Preprocess Features:\n",
    "si = StringIndexer(\n",
    "    inputCols=categorical_features,\n",
    "    outputCols=categorical_features_indexed,\n",
    "    handleInvalid='skip'\n",
    ")\n",
    "va = VectorAssembler(\n",
    "    inputCols=numerical_features + categorical_features_indexed,\n",
    "    outputCol='feature_vector',\n",
    "    handleInvalid='skip'\n",
    ")\n",
    "scaler = MinMaxScaler(\n",
    "    inputCol='feature_vector',\n",
    "    outputCol='scaled_feature_vector'\n",
    ")\n",
    "lr = LinearRegression(\n",
    "    featuresCol='scaled_feature_vector',\n",
    "    labelCol='total_bedrooms',\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    stages = [si,va,scaler,lr]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d0efc8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_fit = pipeline.fit(df_train)\n",
    "test_preds = pl_fit.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "5368c7d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_preds.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea28c05d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
