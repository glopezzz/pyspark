{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "405eaa17",
   "metadata": {},
   "source": [
    "## California Housing Prices\n",
    "Median house prices for California districts derived from the 1990 census.\n",
    "\n",
    "### About Dataset\n",
    "#### Context\n",
    "This is the dataset used in the second chapter of Aurélien Géron's recent book 'Hands-On Machine learning with Scikit-Learn and TensorFlow'. It serves as an excellent introduction to implementing machine learning algorithms because it requires rudimentary data cleaning, has an easily understandable list of variables and sits at an optimal size between being to toyish and too cumbersome.\n",
    "\n",
    "The data contains information from the 1990 California census. So although it may not help you with predicting current housing prices like the Zillow Zestimate dataset, it does provide an accessible introductory dataset for teaching people about the basics of machine learning.\n",
    "\n",
    "### Content\n",
    "The data pertains to the houses found in a given California district and some summary stats about them based on the 1990 census data. Be warned the data aren't cleaned so there are some preprocessing steps required! The columns are as follows, their names are pretty self explanitory:\n",
    "\n",
    "- longitude\n",
    "- latitude\n",
    "- housing_median_age\n",
    "- total_rooms\n",
    "- total_bedrooms\n",
    "- population\n",
    "- households\n",
    "- median_income\n",
    "- median_house_value\n",
    "- ocean_proximity\n",
    "\n",
    "### Acknowledgements\n",
    "This data was initially featured in the following paper:\n",
    "Pace, R. Kelley, and Ronald Barry. \"Sparse spatial autoregressions.\" Statistics & Probability Letters 33.3 (1997): 291-297.\n",
    "\n",
    "and I encountered it in 'Hands-On Machine learning with Scikit-Learn and TensorFlow' by Aurélien Géron.\n",
    "Aurélien Géron wrote:\n",
    "This dataset is a modified version of the California Housing dataset available from:\n",
    "Luís Torgo's page (University of Porto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cd6f164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import kagglehub\n",
    "\n",
    "# # Download latest version\n",
    "# path = kagglehub.dataset_download(\"camnugent/california-housing-prices\")\n",
    "\n",
    "# print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ce91928",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder.appName('California_Housing_Prediction')\n",
    "    .master('local[*]')\n",
    "    .config(\"spark.driver.memory\", '4g')\n",
    "    .getOrCreate()\n",
    ")\n",
    "spark.sparkContext.setLogLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b49fef51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n",
      "|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|ocean_proximity|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n",
      "|  -122.23|   37.88|              41.0|      880.0|         129.0|     322.0|     126.0|       8.3252|          452600.0|       NEAR BAY|\n",
      "|  -122.22|   37.86|              21.0|     7099.0|        1106.0|    2401.0|    1138.0|       8.3014|          358500.0|       NEAR BAY|\n",
      "|  -122.24|   37.85|              52.0|     1467.0|         190.0|     496.0|     177.0|       7.2574|          352100.0|       NEAR BAY|\n",
      "|  -122.25|   37.85|              52.0|     1274.0|         235.0|     558.0|     219.0|       5.6431|          341300.0|       NEAR BAY|\n",
      "|  -122.25|   37.85|              52.0|     1627.0|         280.0|     565.0|     259.0|       3.8462|          342200.0|       NEAR BAY|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('./data/housing.csv', header=True, inferSchema=True)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7b5d225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- housing_median_age: double (nullable = true)\n",
      " |-- total_rooms: double (nullable = true)\n",
      " |-- total_bedrooms: double (nullable = true)\n",
      " |-- population: double (nullable = true)\n",
      " |-- households: double (nullable = true)\n",
      " |-- median_income: double (nullable = true)\n",
      " |-- median_house_value: double (nullable = true)\n",
      " |-- ocean_proximity: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64b74eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 20640\n"
     ]
    }
   ],
   "source": [
    "print('Total rows:',df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "921015d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets crate an unique id column\n",
    "df = df.withColumn('id', F.monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a24594e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+---+\n",
      "|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|ocean_proximity| id|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+---+\n",
      "|        0|       0|                 0|          0|           207|         0|         0|            0|                 0|              0|  0|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check Nulls in the dataframe\n",
    "df.select(\n",
    "    [F.count(F.when(F.col(col).isNull(), 1)).alias(col) for col in df.columns]\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e464e2d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+\n",
      "|count(DISTINCT ocean_proximity)|\n",
      "+-------------------------------+\n",
      "|                              5|\n",
      "+-------------------------------+\n",
      "\n",
      "+---------------+-----+\n",
      "|ocean_proximity|count|\n",
      "+---------------+-----+\n",
      "|         ISLAND|    5|\n",
      "|     NEAR OCEAN| 2658|\n",
      "|       NEAR BAY| 2290|\n",
      "|      <1H OCEAN| 9136|\n",
      "|         INLAND| 6551|\n",
      "+---------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check categorical column uniques\n",
    "df.select(F.count_distinct(F.col('ocean_proximity'))).show()\n",
    "df.groupBy(\"ocean_proximity\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4e94c8",
   "metadata": {},
   "source": [
    "#### To handle the missing values for the 'total_bedrooms' column, I'm going to train a Linear Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "854a7ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the feature types\n",
    "numerical_features = []\n",
    "categorical_features = []\n",
    "for var, type in df.dtypes:\n",
    "    if type in ['int', 'float', 'double']:\n",
    "        numerical_features.append(var)\n",
    "    elif type in ['string']:\n",
    "        categorical_features.append(var)\n",
    "\n",
    "categorical_features_indexed = [col + '_ind' for col in categorical_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5613c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|df_train Nulls|\n",
      "+--------------+\n",
      "|             0|\n",
      "+--------------+\n",
      "\n",
      "+-------------+\n",
      "|df_test Nulls|\n",
      "+-------------+\n",
      "|          207|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We could actually fill the Null values with a linear regression model\n",
    "df_train = df.where(F.col('total_bedrooms').isNotNull())\n",
    "df_test = df.where(F.col('total_bedrooms').isNull())\n",
    "\n",
    "# Ensure there arent any nulls\n",
    "df_train.select(\n",
    "    F.count(F.when(F.col('total_bedrooms').isNull(), 1)).alias('df_train Nulls')\n",
    ").show()\n",
    "df_test.select(\n",
    "    F.count(F.when(F.col('total_bedrooms').isNull(), 1)).alias('df_test Nulls')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0d483a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, MinMaxScaler\n",
    "from pyspark.ml.pipeline import Pipeline\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "numerical_features = [col for col in numerical_features if col != 'total_bedrooms']\n",
    "\n",
    "# Preprocess Features:\n",
    "si = StringIndexer(\n",
    "    inputCols=categorical_features,\n",
    "    outputCols=categorical_features_indexed,\n",
    "    handleInvalid='skip'\n",
    ")\n",
    "va = VectorAssembler(\n",
    "    inputCols=numerical_features + categorical_features_indexed,\n",
    "    outputCol='feature_vector',\n",
    "    handleInvalid='skip'\n",
    ")\n",
    "scaler = MinMaxScaler(\n",
    "    inputCol='feature_vector',\n",
    "    outputCol='scaled_feature_vector'\n",
    ")\n",
    "\n",
    "lr = LinearRegression(\n",
    "    featuresCol='scaled_feature_vector',\n",
    "    labelCol='total_bedrooms',\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    stages = [si,va,scaler,lr]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0efc8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_fit = pipeline.fit(df_train)\n",
    "test_preds = pl_fit.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5368c7d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+----+-------------------+--------------------+---------------------+------------------+\n",
      "|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|ocean_proximity|  id|ocean_proximity_ind|      feature_vector|scaled_feature_vector|        prediction|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+----+-------------------+--------------------+---------------------+------------------+\n",
      "|  -122.16|   37.77|              47.0|     1256.0|          NULL|     570.0|     218.0|        4.375|          161900.0|       NEAR BAY| 290|                3.0|[-122.16,37.77,47...| [0.21812749003984...|210.09994196212418|\n",
      "|  -122.17|   37.75|              38.0|      992.0|          NULL|     732.0|     259.0|       1.6196|           85100.0|       NEAR BAY| 341|                3.0|[-122.17,37.75,38...| [0.21713147410358...|277.95599520080566|\n",
      "|  -122.28|   37.78|              29.0|     5154.0|          NULL|    3741.0|    1273.0|       2.5762|          173400.0|       NEAR BAY| 538|                3.0|[-122.28,37.78,29...| [0.20617529880478...|1283.7172793147918|\n",
      "|  -122.24|   37.75|              45.0|      891.0|          NULL|     384.0|     146.0|       4.9489|          247100.0|       NEAR BAY| 563|                3.0|[-122.24,37.75,45...| [0.21015936254980...|130.61040067228612|\n",
      "|   -122.1|   37.69|              41.0|      746.0|          NULL|     387.0|     161.0|       3.9063|          178400.0|       NEAR BAY| 696|                3.0|[-122.1,37.69,41....| [0.22410358565737...| 152.3991260318658|\n",
      "|  -122.14|   37.67|              37.0|     3342.0|          NULL|    1635.0|     557.0|       4.7933|          186900.0|       NEAR BAY| 738|                3.0|[-122.14,37.67,37...| [0.22011952191235...| 583.8945561872305|\n",
      "|  -121.77|   39.66|              20.0|     3759.0|          NULL|    1705.0|     600.0|        4.712|          158600.0|         INLAND|1097|                1.0|[-121.77,39.66,20...| [0.25697211155378...| 663.7799470030117|\n",
      "|  -121.95|   38.03|               5.0|     5526.0|          NULL|    3207.0|    1012.0|       4.0767|          143100.0|         INLAND|1350|                1.0|[-121.95,38.03,5....| [0.23904382470119...|1073.1925185209902|\n",
      "|  -121.98|   37.96|              22.0|     2987.0|          NULL|    1420.0|     540.0|         3.65|          204100.0|         INLAND|1456|                1.0|[-121.98,37.96,22...| [0.23605577689242...| 589.2978168561984|\n",
      "|  -122.01|   37.94|              23.0|     3741.0|          NULL|    1339.0|     499.0|       6.7061|          322300.0|       NEAR BAY|1493|                3.0|[-122.01,37.94,23...| [0.23306772908366...|  551.245182940138|\n",
      "|  -122.08|   37.88|              26.0|     2947.0|          NULL|     825.0|     626.0|        2.933|           85000.0|       NEAR BAY|1606|                3.0|[-122.08,37.88,26...| [0.22609561752988...| 685.3640633056993|\n",
      "|  -119.75|   36.71|              38.0|     1481.0|          NULL|    1543.0|     372.0|       1.4577|           49800.0|         INLAND|2028|                1.0|[-119.75,36.71,38...| [0.45816733067729...| 391.8673900624635|\n",
      "|  -119.72|   36.76|              23.0|     6403.0|          NULL|    3573.0|    1260.0|       2.3006|           69000.0|         INLAND|2115|                1.0|[-119.72,36.76,23...| [0.46115537848605...|1364.9480015092217|\n",
      "|  -119.78|   36.82|              25.0|     5016.0|          NULL|    2133.0|     928.0|        3.625|           89500.0|         INLAND|2301|                1.0|[-119.78,36.82,25...| [0.45517928286852...|1021.4895471543069|\n",
      "|  -119.73|   36.83|               8.0|     3602.0|          NULL|    1959.0|     580.0|       5.3478|          138800.0|         INLAND|2323|                1.0|[-119.73,36.83,8....| [0.46015936254980...|  620.762480558158|\n",
      "|  -119.69|   36.83|              32.0|     1098.0|          NULL|     726.0|     224.0|       1.4913|           54600.0|         INLAND|2334|                1.0|[-119.69,36.83,32...| [0.46414342629482...| 274.3522728040271|\n",
      "|  -119.68|   36.79|              16.0|     1551.0|          NULL|    1010.0|     292.0|       3.5417|           71300.0|         INLAND|2351|                1.0|[-119.68,36.79,16...| [0.46513944223107...| 314.3574897958901|\n",
      "|  -119.45|   36.61|              24.0|     1302.0|          NULL|     693.0|     243.0|       3.7917|           90500.0|         INLAND|2412|                1.0|[-119.45,36.61,24...| [0.48804780876493...| 263.8217646171245|\n",
      "|  -119.44|   36.58|              37.0|     1054.0|          NULL|     879.0|     257.0|       2.5234|           63500.0|         INLAND|2420|                1.0|[-119.44,36.58,37...| [0.48904382470119...| 273.3667202168456|\n",
      "|  -124.06|   40.86|              34.0|     4183.0|          NULL|    1891.0|     669.0|       3.2216|           98100.0|     NEAR OCEAN|2578|                2.0|[-124.06,40.86,34...| [0.02888446215139...|  743.534393592517|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+----+-------------------+--------------------+---------------------+------------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "test_preds.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f7879a",
   "metadata": {},
   "source": [
    "Now that we have the predicted total_bedrooms, we can use those predictions to complete the original dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea28c05d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+\n",
      "|Null Count for total_bedrooms|\n",
      "+-----------------------------+\n",
      "|                            0|\n",
      "+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = test_preds.select(['id','prediction'])\n",
    "\n",
    "df_with_preds = df.join(predictions, on='id', how='left')\n",
    "\n",
    "df_filled = df_with_preds.withColumn(\n",
    "    \"total_bedrooms\",\n",
    "    F.coalesce(F.col('total_bedrooms'), F.col('prediction'))\n",
    ")\n",
    "\n",
    "df_final = df_filled.drop('prediction')\n",
    "\n",
    "df_final.select(\n",
    "    F.count(F.when(F.col('total_bedrooms').isNaN(),1)).alias('Null Count for total_bedrooms')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c1c8b0",
   "metadata": {},
   "source": [
    "Now it's time to train the final model with all the features. For that I'm going to:\n",
    "1. Divide the dataset into train and test following a stratified sampling technique for the categorical column\n",
    "2. Train a baseline model and a more advanced model\n",
    "3. Compare the models and keep the better one\n",
    "4. Performe some hyperparameter tunning on the improved model\n",
    "5. Train the final tuned model with the whole dataset\n",
    "6. Present to Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "883fe419",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = df_final.select('ocean_proximity').distinct().collect()\n",
    "category_fractions = {category.ocean_proximity : 0.7 for category in categories}\n",
    "\n",
    "train_df = df_final.sampleBy('ocean_proximity', category_fractions, seed=42)\n",
    "test_df = df_final.join(train_df, on='id', how='left_anti')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f59ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|ocean_proximity|count|\n",
      "+---------------+-----+\n",
      "|         ISLAND|    5|\n",
      "|     NEAR OCEAN| 1808|\n",
      "|       NEAR BAY| 1628|\n",
      "|      <1H OCEAN| 6466|\n",
      "|         INLAND| 4602|\n",
      "+---------------+-----+\n",
      "\n",
      "+---------------+-----+\n",
      "|ocean_proximity|count|\n",
      "+---------------+-----+\n",
      "|     NEAR OCEAN|  850|\n",
      "|       NEAR BAY|  662|\n",
      "|      <1H OCEAN| 2670|\n",
      "|         INLAND| 1949|\n",
      "+---------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.groupBy('ocean_proximity').count().show()\n",
    "test_df.groupBy('ocean_proximity').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "026668de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, MinMaxScaler\n",
    "from pyspark.ml.pipeline import Pipeline\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "numerical_features = [col for col in numerical_features if col != 'median_house_value']\n",
    "\n",
    "# Preprocess Features:\n",
    "si = StringIndexer(\n",
    "    inputCols=categorical_features,\n",
    "    outputCols=categorical_features_indexed,\n",
    "    handleInvalid='skip'\n",
    ")\n",
    "va = VectorAssembler(\n",
    "    inputCols=numerical_features + categorical_features_indexed,\n",
    "    outputCol='feature_vector',\n",
    "    handleInvalid='skip'\n",
    ")\n",
    "scaler = MinMaxScaler(\n",
    "    inputCol='feature_vector',\n",
    "    outputCol='scaled_feature_vector'\n",
    ")\n",
    "\n",
    "lr = LinearRegression(\n",
    "    featuresCol='scaled_feature_vector',\n",
    "    labelCol='median_house_value',\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    stages = [si,va,scaler,lr]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
