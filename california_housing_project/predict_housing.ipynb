{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "405eaa17",
   "metadata": {},
   "source": [
    "## California Housing Prices\n",
    "Median house prices for California districts derived from the 1990 census.\n",
    "\n",
    "### About Dataset\n",
    "#### Context\n",
    "This is the dataset used in the second chapter of Aurélien Géron's recent book 'Hands-On Machine learning with Scikit-Learn and TensorFlow'. It serves as an excellent introduction to implementing machine learning algorithms because it requires rudimentary data cleaning, has an easily understandable list of variables and sits at an optimal size between being to toyish and too cumbersome.\n",
    "\n",
    "The data contains information from the 1990 California census. So although it may not help you with predicting current housing prices like the Zillow Zestimate dataset, it does provide an accessible introductory dataset for teaching people about the basics of machine learning.\n",
    "\n",
    "### Content\n",
    "The data pertains to the houses found in a given California district and some summary stats about them based on the 1990 census data. Be warned the data aren't cleaned so there are some preprocessing steps required! The columns are as follows, their names are pretty self explanitory:\n",
    "\n",
    "- longitude\n",
    "- latitude\n",
    "- housing_median_age\n",
    "- total_rooms\n",
    "- total_bedrooms\n",
    "- population\n",
    "- households\n",
    "- median_income\n",
    "- median_house_value\n",
    "- ocean_proximity\n",
    "\n",
    "### Acknowledgements\n",
    "This data was initially featured in the following paper:\n",
    "Pace, R. Kelley, and Ronald Barry. \"Sparse spatial autoregressions.\" Statistics & Probability Letters 33.3 (1997): 291-297.\n",
    "\n",
    "and I encountered it in 'Hands-On Machine learning with Scikit-Learn and TensorFlow' by Aurélien Géron.\n",
    "Aurélien Géron wrote:\n",
    "This dataset is a modified version of the California Housing dataset available from:\n",
    "Luís Torgo's page (University of Porto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cd6f164",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Gabriel\\envs\\pyspark\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\Gabriel\\.cache\\kagglehub\\datasets\\camnugent\\california-housing-prices\\versions\\1\n",
      "Successfully copied file\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"camnugent/california-housing-prices\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n",
    "fullpath = os.path.join(path, os.listdir(path)[0])\n",
    "\n",
    "# Now lets copy it to the project's folder\n",
    "destination_folder = Path('data')\n",
    "\n",
    "try:\n",
    "    destination_folder.mkdir(parents=True, exist_ok=True)\n",
    "    shutil.copy(fullpath, destination_folder)\n",
    "    print(f\"Successfully copied file\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ce91928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using IP address:  192.168.15.7\n"
     ]
    },
    {
     "ename": "Py4JError",
     "evalue": "An error occurred while calling None.org.apache.spark.sql.SparkSession. Trace:\npy4j.Py4JException: Constructor org.apache.spark.sql.SparkSession([class org.apache.spark.SparkContext, class java.util.HashMap]) does not exist\r\n\tat py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:180)\r\n\tat py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:197)\r\n\tat py4j.Gateway.invoke(Gateway.java:237)\r\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\r\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m     21\u001b[39m     s.close()\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUsing IP address: \u001b[39m\u001b[33m\"\u001b[39m,ip)\n\u001b[32m     24\u001b[39m spark = (\n\u001b[32m     25\u001b[39m     \u001b[43mSparkSession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuilder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mappName\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mCalifornia_Housing_Prediction\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mmaster\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlocal[4]\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspark.driver.host\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlocalhost\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspark.driver.bindAddress\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mip\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspark.driver.memory\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m4g\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mspark.python.worker.faulthandler.enabled\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtrue\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m \u001b[43m    \u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetOrCreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m )\n\u001b[32m     33\u001b[39m spark.sparkContext.setLogLevel(\u001b[33m'\u001b[39m\u001b[33mERROR\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gabriel\\envs\\pyspark\\Lib\\site-packages\\pyspark\\sql\\session.py:500\u001b[39m, in \u001b[36mSparkSession.Builder.getOrCreate\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    497\u001b[39m     sc = SparkContext.getOrCreate(sparkConf)\n\u001b[32m    498\u001b[39m     \u001b[38;5;66;03m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[32m    499\u001b[39m     \u001b[38;5;66;03m# by all sessions.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m500\u001b[39m     session = \u001b[43mSparkSession\u001b[49m\u001b[43m(\u001b[49m\u001b[43msc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    502\u001b[39m     \u001b[38;5;28mgetattr\u001b[39m(\n\u001b[32m    503\u001b[39m         \u001b[38;5;28mgetattr\u001b[39m(session._jvm, \u001b[33m\"\u001b[39m\u001b[33mSparkSession$\u001b[39m\u001b[33m\"\u001b[39m), \u001b[33m\"\u001b[39m\u001b[33mMODULE$\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    504\u001b[39m     ).applyModifiableSettings(session._jsparkSession, \u001b[38;5;28mself\u001b[39m._options)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gabriel\\envs\\pyspark\\Lib\\site-packages\\pyspark\\sql\\session.py:589\u001b[39m, in \u001b[36mSparkSession.__init__\u001b[39m\u001b[34m(self, sparkContext, jsparkSession, options)\u001b[39m\n\u001b[32m    585\u001b[39m         \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m._jvm, \u001b[33m\"\u001b[39m\u001b[33mSparkSession$\u001b[39m\u001b[33m\"\u001b[39m), \u001b[33m\"\u001b[39m\u001b[33mMODULE$\u001b[39m\u001b[33m\"\u001b[39m).applyModifiableSettings(\n\u001b[32m    586\u001b[39m             jsparkSession, options\n\u001b[32m    587\u001b[39m         )\n\u001b[32m    588\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m         jsparkSession = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jvm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mSparkSession\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jsc\u001b[49m\u001b[43m.\u001b[49m\u001b[43msc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    590\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    591\u001b[39m     \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m._jvm, \u001b[33m\"\u001b[39m\u001b[33mSparkSession$\u001b[39m\u001b[33m\"\u001b[39m), \u001b[33m\"\u001b[39m\u001b[33mMODULE$\u001b[39m\u001b[33m\"\u001b[39m).applyModifiableSettings(\n\u001b[32m    592\u001b[39m         jsparkSession, options\n\u001b[32m    593\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gabriel\\envs\\pyspark\\Lib\\site-packages\\py4j\\java_gateway.py:1587\u001b[39m, in \u001b[36mJavaClass.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1581\u001b[39m command = proto.CONSTRUCTOR_COMMAND_NAME +\\\n\u001b[32m   1582\u001b[39m     \u001b[38;5;28mself\u001b[39m._command_header +\\\n\u001b[32m   1583\u001b[39m     args_command +\\\n\u001b[32m   1584\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1586\u001b[39m answer = \u001b[38;5;28mself\u001b[39m._gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1587\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1588\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_gateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fqn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1590\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1591\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gabriel\\envs\\pyspark\\Lib\\site-packages\\py4j\\protocol.py:330\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    326\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    327\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    328\u001b[39m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    329\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m330\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    331\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    332\u001b[39m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n\u001b[32m    333\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    334\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    335\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    336\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name))\n",
      "\u001b[31mPy4JError\u001b[39m: An error occurred while calling None.org.apache.spark.sql.SparkSession. Trace:\npy4j.Py4JException: Constructor org.apache.spark.sql.SparkSession([class org.apache.spark.SparkContext, class java.util.HashMap]) does not exist\r\n\tat py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:180)\r\n\tat py4j.reflection.ReflectionEngine.getConstructor(ReflectionEngine.java:197)\r\n\tat py4j.Gateway.invoke(Gateway.java:237)\r\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\r\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\n\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import sys\n",
    "\n",
    "# Set the Python executable for PySpark workers to match the current one\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "import socket\n",
    "\n",
    "\n",
    "try:\n",
    "    s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n",
    "    s.settimeout(0)\n",
    "    s.connect(('8.8.8.8', 80))\n",
    "    ip = s.getsockname()[0]\n",
    "except Exception:\n",
    "    ip = \"127.0.0.1\"\n",
    "finally:\n",
    "    s.close()\n",
    "\n",
    "print(f\"Using IP address: \",ip)\n",
    "spark = (\n",
    "    SparkSession.builder.appName('California_Housing_Prediction')\n",
    "    .master('local[4]')\n",
    "    .config(\"spark.driver.host\", \"localhost\")\n",
    "    .config(\"spark.driver.bindAddress\", ip)\n",
    "    .config(\"spark.driver.memory\", '4g')\n",
    "    .config(\"spark.python.worker.faulthandler.enabled\", \"true\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "spark.sparkContext.setLogLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78213869",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Gabriel\\\\envs\\\\pyspark\\\\Scripts\\\\python.exe'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49fef51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n",
      "|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|ocean_proximity|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n",
      "|  -122.23|   37.88|              41.0|      880.0|         129.0|     322.0|     126.0|       8.3252|          452600.0|       NEAR BAY|\n",
      "|  -122.22|   37.86|              21.0|     7099.0|        1106.0|    2401.0|    1138.0|       8.3014|          358500.0|       NEAR BAY|\n",
      "|  -122.24|   37.85|              52.0|     1467.0|         190.0|     496.0|     177.0|       7.2574|          352100.0|       NEAR BAY|\n",
      "|  -122.25|   37.85|              52.0|     1274.0|         235.0|     558.0|     219.0|       5.6431|          341300.0|       NEAR BAY|\n",
      "|  -122.25|   37.85|              52.0|     1627.0|         280.0|     565.0|     259.0|       3.8462|          342200.0|       NEAR BAY|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('./data/housing.csv', header=True, inferSchema=True)\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b5d225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- housing_median_age: double (nullable = true)\n",
      " |-- total_rooms: double (nullable = true)\n",
      " |-- total_bedrooms: double (nullable = true)\n",
      " |-- population: double (nullable = true)\n",
      " |-- households: double (nullable = true)\n",
      " |-- median_income: double (nullable = true)\n",
      " |-- median_house_value: double (nullable = true)\n",
      " |-- ocean_proximity: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b74eac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows: 20640\n"
     ]
    }
   ],
   "source": [
    "print('Total rows:',df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921015d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets crate an unique id column\n",
    "df = df.withColumn('id', F.monotonically_increasing_id())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24594e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+---+\n",
      "|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|ocean_proximity| id|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+---+\n",
      "|        0|       0|                 0|          0|           207|         0|         0|            0|                 0|              0|  0|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check Nulls in the dataframe\n",
    "df.select(\n",
    "    [F.count(F.when(F.col(col).isNull(), 1)).alias(col) for col in df.columns]\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e464e2d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------+\n",
      "|count(DISTINCT ocean_proximity)|\n",
      "+-------------------------------+\n",
      "|                              5|\n",
      "+-------------------------------+\n",
      "\n",
      "+---------------+-----+\n",
      "|ocean_proximity|count|\n",
      "+---------------+-----+\n",
      "|         ISLAND|    5|\n",
      "|     NEAR OCEAN| 2658|\n",
      "|       NEAR BAY| 2290|\n",
      "|      <1H OCEAN| 9136|\n",
      "|         INLAND| 6551|\n",
      "+---------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check categorical column uniques\n",
    "df.select(F.count_distinct(F.col('ocean_proximity'))).show()\n",
    "df.groupBy(\"ocean_proximity\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4e94c8",
   "metadata": {},
   "source": [
    "#### To handle the missing values for the 'total_bedrooms' column, I'm going to train a Linear Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854a7ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the feature types\n",
    "numerical_features = []\n",
    "categorical_features = []\n",
    "for var, type in df.dtypes:\n",
    "    if type in ['int', 'float', 'double']:\n",
    "        numerical_features.append(var)\n",
    "    elif type in ['string']:\n",
    "        categorical_features.append(var)\n",
    "\n",
    "categorical_features_indexed = [col + '_ind' for col in categorical_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5613c7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|df_train Nulls|\n",
      "+--------------+\n",
      "|             0|\n",
      "+--------------+\n",
      "\n",
      "+-------------+\n",
      "|df_test Nulls|\n",
      "+-------------+\n",
      "|          207|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We could actually fill the Null values with a linear regression model\n",
    "df_train = df.where(F.col('total_bedrooms').isNotNull())\n",
    "df_test = df.where(F.col('total_bedrooms').isNull())\n",
    "\n",
    "# Ensure there arent any nulls\n",
    "df_train.select(\n",
    "    F.count(F.when(F.col('total_bedrooms').isNull(), 1)).alias('df_train Nulls')\n",
    ").show()\n",
    "df_test.select(\n",
    "    F.count(F.when(F.col('total_bedrooms').isNull(), 1)).alias('df_test Nulls')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d483a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, MinMaxScaler\n",
    "from pyspark.ml.pipeline import Pipeline\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "numerical_features = [col for col in numerical_features if col != 'total_bedrooms']\n",
    "\n",
    "# Preprocess Features:\n",
    "si = StringIndexer(\n",
    "    inputCols=categorical_features,\n",
    "    outputCols=categorical_features_indexed,\n",
    "    handleInvalid='skip'\n",
    ")\n",
    "va = VectorAssembler(\n",
    "    inputCols=numerical_features + categorical_features_indexed,\n",
    "    outputCol='feature_vector',\n",
    "    handleInvalid='skip'\n",
    ")\n",
    "scaler = MinMaxScaler(\n",
    "    inputCol='feature_vector',\n",
    "    outputCol='scaled_feature_vector'\n",
    ")\n",
    "\n",
    "lr = LinearRegression(\n",
    "    featuresCol='scaled_feature_vector',\n",
    "    labelCol='total_bedrooms',\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    stages = [si,va,scaler,lr]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0efc8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl_fit = pipeline.fit(df_train)\n",
    "test_preds = pl_fit.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5368c7d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+----+-------------------+--------------------+---------------------+------------------+\n",
      "|longitude|latitude|housing_median_age|total_rooms|total_bedrooms|population|households|median_income|median_house_value|ocean_proximity|  id|ocean_proximity_ind|      feature_vector|scaled_feature_vector|        prediction|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+----+-------------------+--------------------+---------------------+------------------+\n",
      "|  -122.16|   37.77|              47.0|     1256.0|          NULL|     570.0|     218.0|        4.375|          161900.0|       NEAR BAY| 290|                3.0|[-122.16,37.77,47...| [0.21812749003984...|210.09994196212418|\n",
      "|  -122.17|   37.75|              38.0|      992.0|          NULL|     732.0|     259.0|       1.6196|           85100.0|       NEAR BAY| 341|                3.0|[-122.17,37.75,38...| [0.21713147410358...|277.95599520080566|\n",
      "|  -122.28|   37.78|              29.0|     5154.0|          NULL|    3741.0|    1273.0|       2.5762|          173400.0|       NEAR BAY| 538|                3.0|[-122.28,37.78,29...| [0.20617529880478...| 1283.717279314792|\n",
      "|  -122.24|   37.75|              45.0|      891.0|          NULL|     384.0|     146.0|       4.9489|          247100.0|       NEAR BAY| 563|                3.0|[-122.24,37.75,45...| [0.21015936254980...|130.61040067228615|\n",
      "|   -122.1|   37.69|              41.0|      746.0|          NULL|     387.0|     161.0|       3.9063|          178400.0|       NEAR BAY| 696|                3.0|[-122.1,37.69,41....| [0.22410358565737...|152.39912603186576|\n",
      "|  -122.14|   37.67|              37.0|     3342.0|          NULL|    1635.0|     557.0|       4.7933|          186900.0|       NEAR BAY| 738|                3.0|[-122.14,37.67,37...| [0.22011952191235...| 583.8945561872306|\n",
      "|  -121.77|   39.66|              20.0|     3759.0|          NULL|    1705.0|     600.0|        4.712|          158600.0|         INLAND|1097|                1.0|[-121.77,39.66,20...| [0.25697211155378...| 663.7799470030116|\n",
      "|  -121.95|   38.03|               5.0|     5526.0|          NULL|    3207.0|    1012.0|       4.0767|          143100.0|         INLAND|1350|                1.0|[-121.95,38.03,5....| [0.23904382470119...|1073.1925185209905|\n",
      "|  -121.98|   37.96|              22.0|     2987.0|          NULL|    1420.0|     540.0|         3.65|          204100.0|         INLAND|1456|                1.0|[-121.98,37.96,22...| [0.23605577689242...| 589.2978168561986|\n",
      "|  -122.01|   37.94|              23.0|     3741.0|          NULL|    1339.0|     499.0|       6.7061|          322300.0|       NEAR BAY|1493|                3.0|[-122.01,37.94,23...| [0.23306772908366...|  551.245182940138|\n",
      "|  -122.08|   37.88|              26.0|     2947.0|          NULL|     825.0|     626.0|        2.933|           85000.0|       NEAR BAY|1606|                3.0|[-122.08,37.88,26...| [0.22609561752988...| 685.3640633056993|\n",
      "|  -119.75|   36.71|              38.0|     1481.0|          NULL|    1543.0|     372.0|       1.4577|           49800.0|         INLAND|2028|                1.0|[-119.75,36.71,38...| [0.45816733067729...| 391.8673900624634|\n",
      "|  -119.72|   36.76|              23.0|     6403.0|          NULL|    3573.0|    1260.0|       2.3006|           69000.0|         INLAND|2115|                1.0|[-119.72,36.76,23...| [0.46115537848605...|1364.9480015092213|\n",
      "|  -119.78|   36.82|              25.0|     5016.0|          NULL|    2133.0|     928.0|        3.625|           89500.0|         INLAND|2301|                1.0|[-119.78,36.82,25...| [0.45517928286852...|1021.4895471543069|\n",
      "|  -119.73|   36.83|               8.0|     3602.0|          NULL|    1959.0|     580.0|       5.3478|          138800.0|         INLAND|2323|                1.0|[-119.73,36.83,8....| [0.46015936254980...|  620.762480558158|\n",
      "|  -119.69|   36.83|              32.0|     1098.0|          NULL|     726.0|     224.0|       1.4913|           54600.0|         INLAND|2334|                1.0|[-119.69,36.83,32...| [0.46414342629482...| 274.3522728040271|\n",
      "|  -119.68|   36.79|              16.0|     1551.0|          NULL|    1010.0|     292.0|       3.5417|           71300.0|         INLAND|2351|                1.0|[-119.68,36.79,16...| [0.46513944223107...|314.35748979589016|\n",
      "|  -119.45|   36.61|              24.0|     1302.0|          NULL|     693.0|     243.0|       3.7917|           90500.0|         INLAND|2412|                1.0|[-119.45,36.61,24...| [0.48804780876493...| 263.8217646171244|\n",
      "|  -119.44|   36.58|              37.0|     1054.0|          NULL|     879.0|     257.0|       2.5234|           63500.0|         INLAND|2420|                1.0|[-119.44,36.58,37...| [0.48904382470119...| 273.3667202168456|\n",
      "|  -124.06|   40.86|              34.0|     4183.0|          NULL|    1891.0|     669.0|       3.2216|           98100.0|     NEAR OCEAN|2578|                2.0|[-124.06,40.86,34...| [0.02888446215139...| 743.5343935925171|\n",
      "+---------+--------+------------------+-----------+--------------+----------+----------+-------------+------------------+---------------+----+-------------------+--------------------+---------------------+------------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "test_preds.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f7879a",
   "metadata": {},
   "source": [
    "Now that we have the predicted total_bedrooms, we can use those predictions to complete the original dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea28c05d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+\n",
      "|Null Count for total_bedrooms|\n",
      "+-----------------------------+\n",
      "|                            0|\n",
      "+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = test_preds.select(['id','prediction'])\n",
    "\n",
    "df_with_preds = df.join(predictions, on='id', how='left')\n",
    "\n",
    "df_filled = df_with_preds.withColumn(\n",
    "    \"total_bedrooms\",\n",
    "    F.coalesce(F.col('total_bedrooms'), F.col('prediction'))\n",
    ")\n",
    "\n",
    "df_final = df_filled.drop('prediction')\n",
    "\n",
    "df_final.select(\n",
    "    F.count(F.when(F.col('total_bedrooms').isNaN(),1)).alias('Null Count for total_bedrooms')\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c1c8b0",
   "metadata": {},
   "source": [
    "Now it's time to train the final model with all the features. For that I'm going to:\n",
    "1. Divide the dataset into train and test following a stratified sampling technique for the categorical column\n",
    "2. Train a baseline model and a more advanced model\n",
    "3. Compare the models and keep the better one\n",
    "4. Performe some hyperparameter tunning on the improved model\n",
    "5. Train the final tuned model with the whole dataset\n",
    "6. Present to Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883fe419",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = df_final.select('ocean_proximity').distinct().collect()\n",
    "category_fractions = {category.ocean_proximity : 0.7 for category in categories}\n",
    "\n",
    "train_df = df_final.sampleBy('ocean_proximity', category_fractions, seed=42)\n",
    "test_df = df_final.join(train_df, on='id', how='left_anti')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f59ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|ocean_proximity|count|\n",
      "+---------------+-----+\n",
      "|         ISLAND|    5|\n",
      "|     NEAR OCEAN| 1808|\n",
      "|       NEAR BAY| 1628|\n",
      "|      <1H OCEAN| 6466|\n",
      "|         INLAND| 4602|\n",
      "+---------------+-----+\n",
      "\n",
      "+---------------+-----+\n",
      "|ocean_proximity|count|\n",
      "+---------------+-----+\n",
      "|     NEAR OCEAN|  850|\n",
      "|       NEAR BAY|  662|\n",
      "|      <1H OCEAN| 2670|\n",
      "|         INLAND| 1949|\n",
      "+---------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_df.groupBy('ocean_proximity').count().show()\n",
    "test_df.groupBy('ocean_proximity').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026668de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, MinMaxScaler\n",
    "from pyspark.ml.pipeline import Pipeline\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from xgboost.spark import SparkXGBRegressor\n",
    "\n",
    "numerical_features = [col for col in numerical_features if col != 'median_house_value']\n",
    "\n",
    "# Preprocess Features:\n",
    "si = StringIndexer(\n",
    "    inputCols=categorical_features,\n",
    "    outputCols=categorical_features_indexed,\n",
    "    handleInvalid='skip'\n",
    ")\n",
    "va = VectorAssembler(\n",
    "    inputCols=numerical_features + categorical_features_indexed,\n",
    "    outputCol='feature_vector',\n",
    "    handleInvalid='skip'\n",
    ")\n",
    "scaler = MinMaxScaler(\n",
    "    inputCol='feature_vector',\n",
    "    outputCol='scaled_feature_vector'\n",
    ")\n",
    " # === Linear Regression ===\n",
    "lr = LinearRegression(\n",
    "    featuresCol='scaled_feature_vector',\n",
    "    labelCol='median_house_value',\n",
    ")\n",
    "\n",
    "lr_pipeline = Pipeline(\n",
    "    stages = [si,va,scaler,lr]\n",
    ")\n",
    "\n",
    " # === XGB Regression ===\n",
    "xgb = SparkXGBRegressor(\n",
    "    features_col='scaled_feature_vector',\n",
    "    label_col='median_house_value',\n",
    ")\n",
    "\n",
    "xgb_pipeline = Pipeline(\n",
    "    stages = [si,va,scaler,xgb]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed044f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-27 20:58:51,552 INFO XGBoost-PySpark: _fit Running xgboost-3.0.5 on 1 workers with\n",
      "\tbooster params: {'objective': 'reg:squarederror', 'device': 'cpu', 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Could not recover from a failed barrier ResultStage. Most recent failure reason: Stage failed because barrier task ResultTask(70, 0) finished unsuccessfully.\norg.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:624)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:599)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\r\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:123)\r\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:532)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:601)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:91)\r\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:82)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeNextElementToStream(PythonRDD.scala:334)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeNextInputToStream(PythonRunner.scala:906)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderInputStream.writeAdditionalInputToPythonWorker(PythonRunner.scala:844)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderInputStream.read(PythonRunner.scala:767)\r\n\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:244)\r\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:263)\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:393)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:933)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:925)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:532)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.mutable.Growable.addAll(Growable.scala:61)\r\n\tat scala.collection.mutable.Growable.addAll$(Growable.scala:57)\r\n\tat scala.collection.mutable.ArrayBuilder.addAll(ArrayBuilder.scala:75)\r\n\tat scala.collection.IterableOnceOps.toArray(IterableOnce.scala:1505)\r\n\tat scala.collection.IterableOnceOps.toArray$(IterableOnce.scala:1498)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1057)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2524)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\nCaused by: java.io.EOFException\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:398)\r\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:104)\r\n\t... 38 more\r\n\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:2935)\r\n\tat scala.Option.getOrElse(Option.scala:201)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2935)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2927)\r\n\tat scala.collection.immutable.List.foreach(List.scala:334)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2927)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:2283)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3201)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3141)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3130)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1009)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2484)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2505)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2524)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2549)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1057)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:417)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1056)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:203)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPy4JJavaError\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# fitted_lr_pl = lr_pipeline.fit(train_df)\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# lr_prediction_df = fitted_lr_pl.transform(test_df)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m fitted_xgb_pl = \u001b[43mxgb_pipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m xgb_prediction_df = fitted_xgb_pl.transform(test_df)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gabriel\\envs\\pyspark\\Lib\\site-packages\\pyspark\\ml\\base.py:203\u001b[39m, in \u001b[36mEstimator.fit\u001b[39m\u001b[34m(self, dataset, params)\u001b[39m\n\u001b[32m    201\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.copy(params)._fit(dataset)\n\u001b[32m    202\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    205\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    206\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    207\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m % \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[32m    208\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gabriel\\envs\\pyspark\\Lib\\site-packages\\pyspark\\ml\\pipeline.py:138\u001b[39m, in \u001b[36mPipeline._fit\u001b[39m\u001b[34m(self, dataset)\u001b[39m\n\u001b[32m    136\u001b[39m     dataset = stage.transform(dataset)\n\u001b[32m    137\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# must be an Estimator\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m     model = \u001b[43mstage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    139\u001b[39m     transformers.append(model)\n\u001b[32m    140\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i < indexOfLastEstimator:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gabriel\\envs\\pyspark\\Lib\\site-packages\\pyspark\\ml\\base.py:203\u001b[39m, in \u001b[36mEstimator.fit\u001b[39m\u001b[34m(self, dataset, params)\u001b[39m\n\u001b[32m    201\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.copy(params)._fit(dataset)\n\u001b[32m    202\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    205\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    206\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    207\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m % \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[32m    208\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gabriel\\envs\\pyspark\\Lib\\site-packages\\xgboost\\spark\\core.py:1194\u001b[39m, in \u001b[36m_SparkXGBEstimator._fit\u001b[39m\u001b[34m(self, dataset)\u001b[39m\n\u001b[32m   1181\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m data[\u001b[32m0\u001b[39m], data[\u001b[32m1\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m.join(data[\u001b[32m2\u001b[39m:])\n\u001b[32m   1183\u001b[39m get_logger(_LOG_TAG).info(\n\u001b[32m   1184\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mRunning xgboost-\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m workers with\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1185\u001b[39m     \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33mbooster params: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1192\u001b[39m     dmatrix_kwargs,\n\u001b[32m   1193\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1194\u001b[39m (evals_result, config, booster) = \u001b[43m_run_job\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1195\u001b[39m get_logger(_LOG_TAG).info(\u001b[33m\"\u001b[39m\u001b[33mFinished xgboost training!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1197\u001b[39m result_xgb_model = \u001b[38;5;28mself\u001b[39m._convert_to_sklearn_model(\n\u001b[32m   1198\u001b[39m     \u001b[38;5;28mbytearray\u001b[39m(booster, \u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m), config\n\u001b[32m   1199\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gabriel\\envs\\pyspark\\Lib\\site-packages\\xgboost\\spark\\core.py:1179\u001b[39m, in \u001b[36m_SparkXGBEstimator._fit.<locals>._run_job\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1170\u001b[39m rdd = (\n\u001b[32m   1171\u001b[39m     dataset.mapInPandas(\n\u001b[32m   1172\u001b[39m         _train_booster,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1176\u001b[39m     .mapPartitions(\u001b[38;5;28;01mlambda\u001b[39;00m x: x)\n\u001b[32m   1177\u001b[39m )\n\u001b[32m   1178\u001b[39m rdd_with_resource = \u001b[38;5;28mself\u001b[39m._try_stage_level_scheduling(rdd)\n\u001b[32m-> \u001b[39m\u001b[32m1179\u001b[39m ret = \u001b[43mrdd_with_resource\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1180\u001b[39m data = [v[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m ret]\n\u001b[32m   1181\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m data[\u001b[32m0\u001b[39m], data[\u001b[32m1\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m.join(data[\u001b[32m2\u001b[39m:])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gabriel\\envs\\pyspark\\Lib\\site-packages\\pyspark\\core\\rdd.py:1700\u001b[39m, in \u001b[36mRDD.collect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1698\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m.context):\n\u001b[32m   1699\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ctx._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1700\u001b[39m     sock_info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jvm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPythonRDD\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollectAndServe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jrdd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrdd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1701\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, \u001b[38;5;28mself\u001b[39m._jrdd_deserializer))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gabriel\\envs\\pyspark\\Lib\\site-packages\\py4j\\java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gabriel\\envs\\pyspark\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:282\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    279\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpy4j\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprotocol\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Py4JJavaError\n\u001b[32m    281\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    284\u001b[39m     converted = convert_exception(e.java_exception)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gabriel\\envs\\pyspark\\Lib\\site-packages\\py4j\\protocol.py:327\u001b[39m, in \u001b[36mget_return_value\u001b[39m\u001b[34m(answer, gateway_client, target_id, name)\u001b[39m\n\u001b[32m    325\u001b[39m value = OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[32m2\u001b[39m:], gateway_client)\n\u001b[32m    326\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[32m1\u001b[39m] == REFERENCE_TYPE:\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[32m    328\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    329\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name), value)\n\u001b[32m    330\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    331\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[32m    332\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.\n\u001b[32m    333\u001b[39m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m, name, value))\n",
      "\u001b[31mPy4JJavaError\u001b[39m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Could not recover from a failed barrier ResultStage. Most recent failure reason: Stage failed because barrier task ResultTask(70, 0) finished unsuccessfully.\norg.apache.spark.SparkException: Python worker exited unexpectedly (crashed)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:624)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:599)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\r\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:123)\r\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:101)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:532)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:601)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat scala.collection.Iterator$$anon$9.hasNext(Iterator.scala:583)\r\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:91)\r\n\tat org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.next(SerDeUtil.scala:82)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeNextElementToStream(PythonRDD.scala:334)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$2.writeNextInputToStream(PythonRunner.scala:906)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderInputStream.writeAdditionalInputToPythonWorker(PythonRunner.scala:844)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderInputStream.read(PythonRunner.scala:767)\r\n\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:244)\r\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:263)\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:393)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:933)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:925)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:532)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.mutable.Growable.addAll(Growable.scala:61)\r\n\tat scala.collection.mutable.Growable.addAll$(Growable.scala:57)\r\n\tat scala.collection.mutable.ArrayBuilder.addAll(ArrayBuilder.scala:75)\r\n\tat scala.collection.IterableOnceOps.toArray(IterableOnce.scala:1505)\r\n\tat scala.collection.IterableOnceOps.toArray$(IterableOnce.scala:1498)\r\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1057)\r\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2524)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\r\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\nCaused by: java.io.EOFException\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:398)\r\n\tat org.apache.spark.sql.execution.python.PythonArrowOutput$$anon$1.read(PythonArrowOutput.scala:104)\r\n\t... 38 more\r\n\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$3(DAGScheduler.scala:2935)\r\n\tat scala.Option.getOrElse(Option.scala:201)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2935)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2927)\r\n\tat scala.collection.immutable.List.foreach(List.scala:334)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2927)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:2283)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3201)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3141)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3130)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:50)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1009)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2484)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2505)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2524)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2549)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1057)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:417)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1056)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:203)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\n"
     ]
    }
   ],
   "source": [
    "# fitted_lr_pl = lr_pipeline.fit(train_df)\n",
    "# lr_prediction_df = fitted_lr_pl.transform(test_df)\n",
    "\n",
    "fitted_xgb_pl = xgb_pipeline.fit(train_df)\n",
    "xgb_prediction_df = fitted_xgb_pl.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5eeae4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lr_prediction_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m      3\u001b[39m evaluator = RegressionEvaluator(\n\u001b[32m      4\u001b[39m     predictionCol=\u001b[33m'\u001b[39m\u001b[33mprediction\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      5\u001b[39m     labelCol=\u001b[33m'\u001b[39m\u001b[33mmedian_house_value\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      6\u001b[39m )\n\u001b[32m      8\u001b[39m evaluation_metrics = {\n\u001b[32m      9\u001b[39m     evaluator.metricName: \u001b[33m\"\u001b[39m\u001b[33mr2\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     10\u001b[39m     evaluator.metricName: \u001b[33m\"\u001b[39m\u001b[33mmae\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     11\u001b[39m }\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m lr_evaluation = evaluator.evaluate(\u001b[43mlr_prediction_df\u001b[49m, evaluation_metrics)\n\u001b[32m     14\u001b[39m xgb_evaluation = evaluator.evaluate(xgb_prediction_df, evaluation_metrics)\n",
      "\u001b[31mNameError\u001b[39m: name 'lr_prediction_df' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "evaluator = RegressionEvaluator(\n",
    "    predictionCol='prediction',\n",
    "    labelCol='median_house_value',\n",
    ")\n",
    "\n",
    "evaluation_metrics = {\n",
    "    evaluator.metricName: \"r2\",\n",
    "    evaluator.metricName: \"mae\",\n",
    "}\n",
    "\n",
    "lr_evaluation = evaluator.evaluate(lr_prediction_df, evaluation_metrics)\n",
    "xgb_evaluation = evaluator.evaluate(xgb_prediction_df, evaluation_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808b2eb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
